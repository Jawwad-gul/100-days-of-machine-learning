{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c5c68a-71b5-4887-9434-cab9ab63f909",
   "metadata": {},
   "source": [
    "# [Precision, Recall and F1 Score](https://youtu.be/iK-kdhJ-7yI?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)  \n",
    "\n",
    "\n",
    "![ytss](assets/accuracyFailed.png)  \n",
    "<br>\n",
    "\n",
    "there are two models both classifying the incoming mails as spam and not spam... now there are two kind of possible mistakes. \n",
    "1. **FP** (False Positive): when the mail is not spam but is classified as spam.\n",
    "2. **FN** (False Negatice): when the mail is spam but is classified as not spam.\n",
    "the second mistake is more sever... most important mails could be classified as spam and sent to spam folder or junk folder.thats not optimal whereas it can be somewhat tolerable if spam mailgets into inbox.\n",
    "\n",
    "<br>  \n",
    "In the ss above the accuracy_score will give same (80%) results for both. because diagonal numbers are same and the sum of all is also same.  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision  \n",
    "How many of the predicted positives are actually positive.  \n",
    "\n",
    "![ytss](assets/Precision.png)  \n",
    "\n",
    "its eaesy to find... lets take example from our initial example.. \n",
    "**Model A**:  \n",
    "TP = 100, FP = 30\n",
    "Precision = 100/(100 + 30)\n",
    "\n",
    "\n",
    "**Model B**:  \n",
    "TP = 100, FP = 10  \n",
    "Precision = 100/ (100 + 10)  \n",
    "\n",
    "Since model A has larger number in denominator the resultant will be smaller than Model B. So: A(precision) < B(precision)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Recall.    \n",
    "**Example**:  \n",
    "<br>\n",
    "![ytss](assets/recall1.png)  \n",
    "severe error here is FN. patient had cancer but classified as healthy.  \n",
    "Recall's entry:  \n",
    "it says out of all the people who actually had cancer how many did the model predicted correctly.  \n",
    "TP / (TP + FN)  \n",
    "A: 1000 / (1000 + 200) <br>\n",
    "B: 1000 / (1000 + 500)  \n",
    "A(recall) < B(recall)  \n",
    "\n",
    "## Which to choose?  \n",
    "It depends on the nature of data. if the Type 1 error is more dangerous, high  precision model will be preferred, if Type2 error is more dangerous high recall model will be preferred.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## F1 Score:  \n",
    "Its used when we don't know which kind of error is more dangerous. example. classification on images of dogs and cats.   \n",
    "#### Formula:  2PA / (P + R), where P is Precision, R is Recall... formula is called Harmonic mean.   \n",
    "harmonic mean always stays at the lower side... if p is 60 and r is 100. harmonic mean will be 75, where as is p and r both are 80 and 80, harmonic mean will be 80. now in both cases sum is 160. but harmonic mean changes if one value is smaller.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Multiclass precision  \n",
    "![ytss](assets/multi.png)\n",
    "![ytss](assets/multi2.png)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Multi clas recall and F1 Score  \n",
    "![ytss](assets/recall&F1score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0c2d3-ae7f-4792-82a6-8bca7961b00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
