{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedf36bf-a695-4789-958a-960c1601609a",
   "metadata": {},
   "source": [
    "# [Accuracy and Confusion Metrices](https://youtu.be/c09drtuCS3c?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)  \n",
    "<br>\n",
    "\n",
    "#### accuracy_score\n",
    "![ytss](assets/accuracy.png)  \n",
    "accuracy_score returns the percentage of correct predictions made by model w.r.t actual data.  \n",
    "it can give the percentage of correction and wrongness... but in being wrong it can't tell whats more wrong. \n",
    "<br> Example:<br>\n",
    "1. if the prediction was suposed to be Yes according to data but model said No.\n",
    "2. if the prediction was supposed to be No according to data but model said Yes.\n",
    "These both are mistakes and accuracy_score can't tell which of these mistakes is happening more.\n",
    "Thats where Confusion metrics come in.\n",
    "\n",
    "---\n",
    "\n",
    "#### confusion_matrix\n",
    "Confusion metrics can tell which kind of mistake is occuring.  \n",
    "<br>\n",
    "\n",
    "![ytss](assets/confusion.png)  \n",
    "<br>\n",
    "\n",
    "supposed to be: actual data.  \n",
    "are: predicted by model\n",
    "\n",
    "* In this matrix. 26 values are predicted correctly, were supposed to be 1 and are 1.\n",
    "* 29 values were supposed to be 0 and are 0.\n",
    "* 6 values were supposed to be 1 but are 0.\n",
    "* and no mistake in supposed to be 0 and not being 0.\n",
    "\n",
    "---\n",
    "\n",
    "confustion matrix can give us accuracy score but opposite is not true.  \n",
    "acc = (Tp+TN)/(TP+TN+FP+FN).  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Type 1 and Type 2 Error.  \n",
    "1. Every error that occurs in **False positive** is **Type 1** . meaning model said its 1 but its actually 0.\n",
    "2. Every error that occurs in **False Negative** is **Type 2**. meaning model said its 0 but its actually 1.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## When will accuracy miss behave?  \n",
    "Accuracy is unreliable on imbalanced data because it is dominated by the majority class.\n",
    "A model can predict the majority class every time, achieve very high accuracy, and still completely fail to detect the minority class. Accuracy ignores which class is being predicted correctly and hides poor performance on rare but important cases.  \n",
    "<br>\n",
    "![ytss](assets/accuracy_fails.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac58f5-d1ac-4cbb-ae71-4e208866e483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
