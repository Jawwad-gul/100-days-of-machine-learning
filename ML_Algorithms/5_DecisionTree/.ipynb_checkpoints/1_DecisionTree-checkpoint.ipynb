{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d637ebfd-78e8-4f95-ac25-885dec06592c",
   "metadata": {},
   "source": [
    "# [Decision Tree](https://youtu.be/IZnno-dKgVQ?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)  \n",
    "in simplest prommatical terms, decision tree is nothing but lot of nested if-else statements.   \n",
    "geomatrically, it keeps deviding the data in the coordinates based on a condition.  \n",
    "<br>\n",
    "\n",
    "### Advantages & Disadvantages  \n",
    "* ðŸŸ¢minimal data preprocessing.\n",
    "* ðŸ”´prone to overfitting\n",
    "* ðŸŸ¢intuitively easy to understand and implement.\n",
    "* ðŸŸ¢cost of using the tree for inference is logarithmic (a good thing).\n",
    "* ðŸ”´prone to errors for imbalanced data.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### To understand there are two terms which are must to be known. Entropy and Information Gain.  \n",
    "\n",
    "---\n",
    "\n",
    "### Entropy  \n",
    "Let's understand it through example  \n",
    "**Example 1** <br>\n",
    "\n",
    "![ytss](assets/1_entropy.png)  \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The example here given here is just to increase the understanding... water has three states and now to think which state has more disorder is the task. ofcourse its vapor because molecules have no oder. they are free to move.  <br>\n",
    "\n",
    "**Example 2**  \n",
    "The next example if of the scatter plot. we have to find out which scatter plot of the two has more entropy.   \n",
    "![ytss](Assets/2_entropy.png)  \n",
    "now as we know that entropy is a measure of disorder. means it should be difficult to make accurate prediction with more entropy comapred to less entropy. now if we look carefully we can see that the scatter plot on the left has only few data point of class-green. which means in left there is veri high probability of red being predicted. and only a little chance of green which is not the case in left scatter plot. which means left scatter plot has higher value of entropy than the right plot.   \n",
    "\n",
    "\n",
    "#### We got the gist of it but how to actually calculate it?  \n",
    "\n",
    "\n",
    "### Calculation of Entropy.  \n",
    "![ytss](assets/3_entropy_calc.png)  \n",
    "\n",
    "**Another example but this time with mathematical calculation:**  \n",
    "![ytss](assets/3_entropy_calc2.png)  \n",
    "\n",
    "So now we can clearly tell the more the data is imbalanced the more the entropy. infact let's see if there is **only one class in wholedataset.**  \n",
    "\n",
    "![ytss](assets/3_entropy_calc3.png)  \n",
    "\n",
    "### Entropy for 3 class dataset:  \n",
    "![ytss](assets/4_entropy_calc.png)  \n",
    "\n",
    "**Revision upto this point:**  \n",
    "![ytss](assets/5_entropy_takeaways.png)  \n",
    "<br>  \n",
    "\n",
    "### Entropy for Continuous Variables.  \n",
    "\n",
    "\n",
    "**This method is just a hack to understand for now. later will study it in detail** \n",
    "\n",
    "![ytss](assets/6_entropy.png)   \n",
    "Peaked KDE = everyone standing in one spot â†’ easy to guess â†’ low entropy\n",
    "\n",
    "Flat KDE = people spread everywhere â†’ hard to guess â†’ high entropy\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Information Gain  \n",
    "  \n",
    "![ytss](assets/7_info_gain.png)  \n",
    "\n",
    "<br>  \n",
    "\n",
    "### Let's just look at an example and understand from there.  \n",
    "![ytss](assets/8_infoGain.png)  \n",
    "\n",
    "#### Calucating the information gain takin the outlook as root .  \n",
    "\n",
    "![ytss](assets/9_infoGain.png)\n",
    "![ytss](assets/10_infoGain.png)\n",
    "\n",
    "### if the entropy of a children is 0, it becomes leaf node. no further splitting.\n",
    "\n",
    "![ytss](assets/11_infoGain.png)  \n",
    "\n",
    "\n",
    "## Takeaway:  \n",
    "Now we have calculated the Information Gain for all columns and selected Outlook as the root because it had the highest gain. The algorithm then splits the data into branches for every possible value of that root (Sunny, Overcast, and Rainy). Instead of picking just one child, the algorithm repeats the process for every branch independently. For each branch, we check the filtered data: if it is already 'pure' (all outcomes are the same,entropy=0), it becomes a final leaf node; if not, we calculate the Information Gain for the remaining columns within that specific branch to find the next best splitter. This recursive splitting continues until the entire tree is built.  \n",
    "\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "## Gini Impurity.  \n",
    "Same thing as entropy, same purpose , different formula, and a little bit of difference...  \n",
    "formula to claculate for a binary problem: `1 - (Py^2 - Pn^2)`\n",
    "**Example**  \n",
    "Consider a binary classification data set... with 50% of yes and 50% of no.  \n",
    "Py = 0.5, Pn = 0.5. In this case Entropy is always maximum which is 1. where as in Gini Impurity case.. the maximum value that can be reached is 0.5, won't go higher than that.   \n",
    "so if the data is too large, we should use Gini impurity cuz its fast. whereas if the data is too imbalanced.. we should use entropy because it penalizes even the smaller changes.  \n",
    "\n",
    "---\n",
    "\n",
    "## What to do with numeric data.  \n",
    "Suppose you have a data: \n",
    "|User-rating|Downloaded|\n",
    "|------------|----------|\n",
    "|some rows|yes/No|  \n",
    "![ytss](Assets/12_numericData.png)\n",
    "![ytss](Assets/13_numericData.png)  \n",
    "1. First off sort the dataset based on your numeric column.\n",
    "2. based on every value in the user-rating column, we split the whole data set into two different ones. condition... `f > value1`. now only value1 is not greater than value1. so dataset splits into two different ones. D1 has only first row while D2 has all the others... just like that we split the data into two with all the values.\n",
    "3. After splitting we calculate all of the splitted data's entropy and then weightage average to finally calculate information gain. (entropy(parent) - Weigted_Avg*entropy).\n",
    "![ytss](assets/14_numericData.png)\n",
    "5.  Now we have n number of information gains. so we find the max from them and then split the tree based off of that value lets it was `f > value3`\n",
    "6. Finally we keep doing this until we reach the leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12d548-65dd-4176-ba00-a12f223790ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
