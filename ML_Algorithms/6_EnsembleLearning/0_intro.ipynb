{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01756267-2b6b-4a40-8070-9b2926a3fb40",
   "metadata": {},
   "source": [
    "# [Ensemble Learning](https://youtu.be/bHK1fE_BUms?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)  \n",
    "<br>\n",
    "\n",
    "First thing to understand about Ensemble learning is its literal meaning... it basically means collection... could be collection of dancers, editors, actors or any kind of artists.  \n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "### Core idea  \n",
    "Next thing to understand is the terms **Wisdom of Crowd**: It basically means that crowd knows. if we have reactions of 10000 people on a certain thing we can tell if its a good thing or bad thing based on reactions and can make better decision.\n",
    "<br><br>\n",
    "every model goes through two phases:  \n",
    "1. Training\n",
    "2. Predicting.\n",
    "\n",
    "\n",
    "Here let's discuss prediction first... training will be discussed when learning about types of ensemble learning.  \n",
    "\n",
    "---\n",
    "\n",
    "### Prediction  \n",
    "\n",
    "so as already discussed. the core concept in ensemble learning is wisdom of crowd, we have to bring that into machine learning. how? let's see.\n",
    "<br>\n",
    "\n",
    "![ytss](assets/0_ensemblePrediction.png)  \n",
    "\n",
    "We take multiple trained models and bring them into one place. then make decision based on all of their predictions.  \n",
    "\n",
    "#### Now suppose we have our good old classification problem: |iq|cgpa|placement|. How to predict.  \n",
    "\n",
    "Well first we need to make sure one of the following crieteria has to be ensured:  \n",
    "1. All the models are same and the training data for each of them is different on which they are trained.\n",
    "2. If trainig data is same then all of the models have to be different. e.g: decisiontree, logisticregression,knn,svm. <br>\n",
    "**_(These conditions are same in regression problem as well, just the models will change ofc)_**\n",
    "\n",
    "\n",
    "Now then, how to finalize the prediction?  \n",
    "\n",
    "We take the new query point and give it to all the models. and then finally choose the one answer which majority of models return. like let's say 3 out of 5 models said that the student will get the placement in this case our final answer will be YES. thats it. \n",
    "<br>\n",
    "\n",
    "#### How to do things when its a regression problem, say: |iq|cgpa|LPA|  \n",
    "In this case everything remains same, only difference is when all the models return their predictions, their mean is taken and returned as final prediction.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Most Important Types / Techniques   \n",
    "\n",
    "1. **Voting Ensemle**<br>\n",
    "    Here we take different base models and same data. prediction is done same as discussed in general form.\n",
    "2. **Bagging Ensemble** (Bootstrapped Aggregation) <br>\n",
    "    Here we take different data and feed that as training data to all models. let's say we have data of 1000 students, we take randoml 500 students and give that as D1 to first model. then do it again and give it to second model as D2, and same for third model which got D3. rest is same. return the mojorities prediction as final predicted answer.  **Random forest** is a speific form of Bagging. where models are DecisionTrees.\n",
    "3. **Boosting Ensemble**\n",
    "In boosting we keep the models same and all of the data is given as input to first model. now the first model runs and does its training but while training it also notcies its mistakes. when its done trainig, it gives the data to next model along with the problems that it ran through. so now the next model becomes cautios about those mistakes and reduces them, but can still run through mistakes. so it sends those mistakes and the data to next one... and it keeps on going until the mistakes are reduced to minimum ... I know there are a lot of question and will be answered later in detail **Ada-boost, XGBoost** are specialized cases of bagging... _**Bagging is the most powerful technique used in machine learning**_\n",
    " \n",
    "4. **Stacking Ensemble**<br>\n",
    "in stacking, the setup is almost same as voting ensemble with just one change. we add one extra model. this extra's model's input data is predictions made by initial base models which they made on new incoming query. Going through multiple new queries, eventually this extra model that we added can now tell which one of the initial base models is performing better (ofc giving correct results) more times than others. and then it assigns them weightage. like let's say decision tree was performing better than other models. so it will give it weightage more than other models so its decision gets more preference.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Why Ensemble techniques Work?  \n",
    "\n",
    "Let's look at classification problem first:  \n",
    "![ytss](assets/1_ensembleWorks.png)\n",
    "![ytss](assets/2_ensembleWorks.png)  \n",
    "notice how initially every model made its own decision boudary. and when plotted together it created a very noisy bounday. thats where ensemble does its thing... it unites them and makes one single decision bounday, noise? gone. uncertainity? reduced\n",
    "\n",
    "\n",
    "Same thing for regression as well.:  \n",
    "![ytss](assets/3_ensembleWorks.png)  \n",
    "I'd say its pretty self explanatory  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Benefits  \n",
    "One obvious disadvantage is that now we have to train multiple models and doing so increases the computational complexity. so unless we get some magic out of it we avoid it... well the thing is that we do get a magic out of it.  \n",
    "\n",
    "1. Performance improvement\n",
    "2. Low Bias , Low variance... usually we don't find this quite often in ML algos.\n",
    "3. Robustness.... again this can be covered in low variance but still is mentioned as separate benefit.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## When to use?  \n",
    "AlwaysðŸ˜‚,no reason for not using it. just keep in mind the benefits and disadvantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8d602-3e30-4b4a-80ca-d22957621b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
